{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Working on Session data...\n",
      "0 from 135483\n",
      "10000 from 135483\n",
      "20000 from 135483\n",
      "30000 from 135483\n",
      "40000 from 135483\n",
      "50000 from 135483\n",
      "60000 from 135483\n",
      "70000 from 135483\n",
      "80000 from 135483\n",
      "90000 from 135483\n",
      "100000 from 135483\n",
      "110000 from 135483\n",
      "120000 from 135483\n",
      "130000 from 135483\n",
      "Working on users data...\n",
      "feature engineering finish, output X,y\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime, date\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "print('Loading raw data...')\n",
    "train_users_path='./input/train_users_2.csv'\n",
    "sessions_path='./input/sessions.csv'\n",
    "\n",
    "\n",
    "#########Loading data#############\n",
    "#train_users\n",
    "df_train = pd.read_csv(train_users_path)\n",
    "target = df_train['country_destination']\n",
    "df_train = df_train.drop(['country_destination'], axis=1)\n",
    "\n",
    "#sessions\n",
    "df_sessions = pd.read_csv(sessions_path)\n",
    "df_sessions['id'] = df_sessions['user_id']\n",
    "df_sessions = df_sessions.drop(['user_id'],axis=1)\n",
    "\n",
    "#########Preparing Session data########\n",
    "print('Working on Session data...')\n",
    "#Filling nan with specific value ('NAN')\n",
    "df_sessions.action = df_sessions.action.fillna('NAN')\n",
    "df_sessions.action_type = df_sessions.action_type.fillna('NAN')\n",
    "df_sessions.action_detail = df_sessions.action_detail.fillna('NAN')\n",
    "df_sessions.device_type = df_sessions.device_type.fillna('NAN')\n",
    "\n",
    "#Action values with low frequency are changed to 'OTHER'\n",
    "act_freq = 100  #Threshold for frequency\n",
    "act = dict(zip(*np.unique(df_sessions.action, return_counts=True)))\n",
    "df_sessions.action = df_sessions.action.apply(lambda x: 'OTHER' if act[x] < act_freq else x)\n",
    "\n",
    "#Computing value_counts. These are going to be used in the one-hot encoding\n",
    "#based feature generation (following loop).\n",
    "f_act = df_sessions.action.value_counts().argsort()\n",
    "f_act_detail = df_sessions.action_detail.value_counts().argsort()\n",
    "f_act_type = df_sessions.action_type.value_counts().argsort()\n",
    "f_dev_type = df_sessions.device_type.value_counts().argsort()\n",
    "\n",
    "#grouping session by id. We will compute features from all rows with the same id.\n",
    "dgr_sess = df_sessions.groupby(['id'])\n",
    "\n",
    "#Loop on dgr_sess to create all the features.\n",
    "samples = []\n",
    "cont = 0\n",
    "ln = len(dgr_sess)\n",
    "for g in dgr_sess:\n",
    "    if cont%10000 == 0:\n",
    "        print(\"%s from %s\" %(cont, ln))\n",
    "    gr = g[1]\n",
    "    l = []\n",
    "\n",
    "    #the id\n",
    "    l.append(g[0])\n",
    "\n",
    "    #The actual first feature is the number of values.\n",
    "    l.append(len(gr))\n",
    "\n",
    "    sev = gr.secs_elapsed.fillna(0).values   #These values are used later.\n",
    "\n",
    "    #action features\n",
    "    #(how many times each value occurs, numb of unique values, mean and std)\n",
    "    c_act = [0] * len(f_act)\n",
    "    for i,v in enumerate(gr.action.values):\n",
    "        c_act[f_act[v]] += 1\n",
    "    _, c_act_uqc = np.unique(gr.action.values, return_counts=True)\n",
    "    c_act += [len(c_act_uqc), np.mean(c_act_uqc), np.std(c_act_uqc)]\n",
    "    l = l + c_act\n",
    "\n",
    "    #action_detail features\n",
    "    #(how many times each value occurs, numb of unique values, mean and std)\n",
    "    c_act_detail = [0] * len(f_act_detail)\n",
    "    for i,v in enumerate(gr.action_detail.values):\n",
    "        c_act_detail[f_act_detail[v]] += 1\n",
    "    _, c_act_det_uqc = np.unique(gr.action_detail.values, return_counts=True)\n",
    "    c_act_detail += [len(c_act_det_uqc), np.mean(c_act_det_uqc), np.std(c_act_det_uqc)]\n",
    "    l = l + c_act_detail\n",
    "\n",
    "    #action_type features\n",
    "    #(how many times each value occurs, numb of unique values, mean and std\n",
    "    #+ log of the sum of secs_elapsed for each value)\n",
    "    l_act_type = [0] * len(f_act_type)\n",
    "    c_act_type = [0] * len(f_act_type)\n",
    "    for i,v in enumerate(gr.action_type.values):\n",
    "        l_act_type[f_act_type[v]] += sev[i]\n",
    "        c_act_type[f_act_type[v]] += 1\n",
    "    l_act_type = np.log(1 + np.array(l_act_type)).tolist()\n",
    "    _, c_act_type_uqc = np.unique(gr.action_type.values, return_counts=True)\n",
    "    c_act_type += [len(c_act_type_uqc), np.mean(c_act_type_uqc), np.std(c_act_type_uqc)]\n",
    "    l = l + c_act_type + l_act_type\n",
    "\n",
    "    #device_type features\n",
    "    #(how many times each value occurs, numb of unique values, mean and std)\n",
    "    c_dev_type  = [0] * len(f_dev_type)\n",
    "    for i,v in enumerate(gr.device_type .values):\n",
    "        c_dev_type[f_dev_type[v]] += 1\n",
    "    c_dev_type.append(len(np.unique(gr.device_type.values)))\n",
    "    _, c_dev_type_uqc = np.unique(gr.device_type.values, return_counts=True)\n",
    "    c_dev_type += [len(c_dev_type_uqc), np.mean(c_dev_type_uqc), np.std(c_dev_type_uqc)]\n",
    "    l = l + c_dev_type\n",
    "\n",
    "    #secs_elapsed features\n",
    "    l_secs = [0] * 5\n",
    "    l_log = [0] * 15\n",
    "    if len(sev) > 0:\n",
    "        #Simple statistics about the secs_elapsed values.\n",
    "        l_secs[0] = np.log(1 + np.sum(sev))\n",
    "        l_secs[1] = np.log(1 + np.mean(sev))\n",
    "        l_secs[2] = np.log(1 + np.std(sev))\n",
    "        l_secs[3] = np.log(1 + np.median(sev))\n",
    "        l_secs[4] = l_secs[0] / float(l[1])\n",
    "\n",
    "        #Values are grouped in 15 intervals. Compute the number of values\n",
    "        #in each interval.\n",
    "        log_sev = np.log(1 + sev).astype(int)\n",
    "        l_log = np.bincount(log_sev, minlength=15).tolist()\n",
    "    l = l + l_secs + l_log\n",
    "\n",
    "    #The list l has the feature values of one sample.\n",
    "    samples.append(l)\n",
    "    cont += 1\n",
    "\n",
    "#Creating a dataframe with the computed features\n",
    "col_names = []    #name of the columns\n",
    "for i in range(len(samples[0])-1):\n",
    "    col_names.append('c_' + str(i))\n",
    "#preparing objects\n",
    "samples = np.array(samples)\n",
    "samp_ar = samples[:, 1:].astype(np.float16)\n",
    "samp_id = samples[:, 0]   #The first element in obs is the id of the sample.\n",
    "\n",
    "#creating the dataframe\n",
    "df_agg_sess = pd.DataFrame(samp_ar, columns=col_names)\n",
    "df_agg_sess['id'] = samp_id\n",
    "df_agg_sess.index = df_agg_sess.id\n",
    "\n",
    "#########Working on train and test data#####################\n",
    "print('Working on users data...')\n",
    "#Concatenating df_train and df_test\n",
    "df_tt = df_train\n",
    "df_tt.index = df_tt.id\n",
    "df_tt = df_tt.fillna(-1)  #Inputing this kind of missing value with -1 (missing values in train and test)\n",
    "df_tt = df_tt.replace('-unknown-', -1) #-unknown is another way of missing value, then = -1.\n",
    "\n",
    "########Creating features for train+test\n",
    "#Removing date_first_booking\n",
    "index = df_train.date_first_booking.isnull()\n",
    "num = [1-int(i) for i in index]\n",
    "df_tt['travel'] = num\n",
    "\n",
    "df_tt = df_tt.drop(['date_first_booking'], axis=1)\n",
    "\n",
    "#Number of nulls\n",
    "df_tt['n_null'] = np.array([sum(r == -1) for r in df_tt.values])\n",
    "\n",
    "#date_account_created\n",
    "#(Computing year, month, day, week_number, weekday)\n",
    "dac = np.vstack(df_tt.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\n",
    "df_tt['dac_y'] = dac[:,0]\n",
    "df_tt['dac_m'] = dac[:,1]\n",
    "df_tt['dac_d'] = dac[:,2]\n",
    "dac_dates = [datetime(x[0],x[1],x[2]) for x in dac]\n",
    "df_tt['dac_wn'] = np.array([d.isocalendar()[1] for d in dac_dates])\n",
    "df_tt['dac_w'] = np.array([d.weekday() for d in dac_dates])\n",
    "df_tt_wd = pd.get_dummies(df_tt.dac_w, prefix='dac_w')\n",
    "df_tt = df_tt.drop(['date_account_created', 'dac_w'], axis=1)\n",
    "df_tt = pd.concat((df_tt, df_tt_wd), axis=1)\n",
    "\n",
    "#timestamp_first_active\n",
    "#(Computing year, month, day, hour, week_number, weekday)\n",
    "tfa = np.vstack(df_tt.timestamp_first_active.astype(str).apply(lambda x: list(map(int, [x[:4],x[4:6],x[6:8],x[8:10],x[10:12],x[12:14]]))).values)\n",
    "df_tt['tfa_y'] = tfa[:,0]\n",
    "df_tt['tfa_m'] = tfa[:,1]\n",
    "df_tt['tfa_d'] = tfa[:,2]\n",
    "df_tt['tfa_h'] = tfa[:,3]\n",
    "tfa_dates = [datetime(x[0],x[1],x[2],x[3],x[4],x[5]) for x in tfa]\n",
    "df_tt['tfa_wn'] = np.array([d.isocalendar()[1] for d in tfa_dates])\n",
    "df_tt['tfa_w'] = np.array([d.weekday() for d in tfa_dates])\n",
    "df_tt_wd = pd.get_dummies(df_tt.tfa_w, prefix='tfa_w')\n",
    "df_tt = df_tt.drop(['timestamp_first_active', 'tfa_w'], axis=1)\n",
    "df_tt = pd.concat((df_tt, df_tt_wd), axis=1)\n",
    "\n",
    "#timespans between dates\n",
    "#(Computing absolute number of seconds of difference between dates, sign of the difference)\n",
    "df_tt['dac_tfa_secs'] = np.array([np.log(1+abs((dac_dates[i]-tfa_dates[i]).total_seconds())) for i in range(len(dac_dates))])\n",
    "df_tt['sig_dac_tfa'] = np.array([np.sign((dac_dates[i]-tfa_dates[i]).total_seconds()) for i in range(len(dac_dates))])\n",
    "#    df_tt['dac_tfa_days'] = np.array([np.sign((dac_dates[i]-tfa_dates[i]).days) for i in range(len(dac_dates))])\n",
    "\n",
    "#Comptute seasons from dates\n",
    "#(Computing the season for the two dates)\n",
    "Y = 2000 # dummy leap year to allow input X-02-29 (leap day)\n",
    "seasons = [(0, (date(Y,  1,  1),  date(Y,  3, 20))),  #'winter'\n",
    "           (1, (date(Y,  3, 21),  date(Y,  6, 20))),  #'spring'\n",
    "           (2, (date(Y,  6, 21),  date(Y,  9, 22))),  #'summer'\n",
    "           (3, (date(Y,  9, 23),  date(Y, 12, 20))),  #'autumn'\n",
    "           (0, (date(Y, 12, 21),  date(Y, 12, 31)))]  #'winter'\n",
    "def get_season(dt):\n",
    "    dt = dt.date()\n",
    "    dt = dt.replace(year=Y)\n",
    "    return next(season for season, (start, end) in seasons\n",
    "                if start <= dt <= end)\n",
    "df_tt['season_dac'] = np.array([get_season(dt) for dt in dac_dates])\n",
    "df_tt['season_tfa'] = np.array([get_season(dt) for dt in tfa_dates])\n",
    "#df_all['season_dfb'] = np.array([get_season(dt) for dt in dfb_dates])\n",
    "\n",
    "#Age\n",
    "#(Keeping ages in 14 < age < 99 as OK and grouping others according different kinds of mistakes)\n",
    "av = df_tt.age.values\n",
    "av = np.where(np.logical_and(av<2000, av>1900), 2014-av, av) #This are birthdays instead of age (estimating age by doing 2014 - value)\n",
    "av = np.where(np.logical_and(av<14, av>0), 4, av) #Using specific value=4 for age values below 14\n",
    "av = np.where(np.logical_and(av<2016, av>2010), 9, av) #This is the current year insted of age (using specific value = 9)\n",
    "av = np.where(av > 99, 110, av)  #Using specific value=110 for age values above 99\n",
    "df_tt['age'] = av\n",
    "\n",
    "#AgeRange\n",
    "#(One-hot encoding of the edge according these intervals)\n",
    "interv =  [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 100]\n",
    "def get_interv_value(age):\n",
    "    iv = 20\n",
    "    for i in range(len(interv)):\n",
    "        if age < interv[i]:\n",
    "            iv = i\n",
    "            break\n",
    "    return iv\n",
    "df_tt['age_interv'] = df_tt.age.apply(lambda x: get_interv_value(x))\n",
    "df_tt_ai = pd.get_dummies(df_tt.age_interv, prefix='age_interv')\n",
    "df_tt = df_tt.drop(['age_interv'], axis=1)\n",
    "df_tt = pd.concat((df_tt, df_tt_ai), axis=1)\n",
    "\n",
    "#One-hot-encoding features\n",
    "ohe_feats = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "for f in ohe_feats:\n",
    "    df_tt_dummy = pd.get_dummies(df_tt[f], prefix=f)\n",
    "    df_tt = df_tt.drop([f], axis=1)\n",
    "    df_tt = pd.concat((df_tt, df_tt_dummy), axis=1)\n",
    "\n",
    "######Merging train-test with session data#################\n",
    "df_all = pd.merge(df_tt, df_agg_sess, how='left')\n",
    "df_all = df_all.drop(['id'], axis=1)\n",
    "df_all = df_all.fillna(-2)  #Missing features for samples without sesssion data.\n",
    "#All types of null\n",
    "df_all['all_null'] = np.array([sum(r<0) for r in df_all.values])\n",
    "\n",
    "\n",
    "######Computing X, y ################\n",
    "X = df_all.values\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(target.values)\n",
    "\n",
    "print('feature engineering finish, output X,y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAADDCAYAAABpjB/1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11faa0290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "pca = decomposition.PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.axes([.2, .2, .7, .7])\n",
    "plt.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance_ratio_')\n",
    "plt.xlim(xmax=10)\n",
    "\n",
    "# from plot n=4\n",
    "n_components = 4\n",
    "pca = decomposition.PCA(n_components=n_components)\n",
    "X_pca = pca.fit(X).transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"X_pca.csv\", X_pca, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: the csv files are saved into ./feature_engineering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, test_size=0.25)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_tmp, y_tmp, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input layer to 1st hidden layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If previous \"feature_engineering\" code not being executed, read previously stored file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 12\n",
    "random_state = 1\n",
    "\n",
    "\n",
    "names = [\"Logistic\", \"Nearest Neighbors\", \"XGBoost\", \"GBC\",\n",
    "         \"Random Forest\", \"AdaBoost\",\n",
    "         \"Quadratic Discriminant Analysis\",\n",
    "         \"Linear Discriminant Analysis\"]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(random_state=random_state),\n",
    "    KNeighborsClassifier(n_neighbors=100),\n",
    "    XGBClassifier(max_depth=6, learning_rate=0.3, n_estimators=25,\n",
    "                  objective='multi:softprob', subsample=0.5, colsample_bytree=0.5, seed=0),\n",
    "    GradientBoostingClassifier(n_estimators=50,\n",
    "                               random_state=random_state),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=20),\n",
    "    AdaBoostClassifier(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LinearDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_valid = []\n",
    "p_test = []\n",
    "\n",
    "print('Individual classifiers (1st hidden layer)')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "\n",
    "for nm, clf in zip(names, classifiers):\n",
    "#First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "    clf.fit(X_train, y_train)\n",
    "    yv = clf.predict_proba(X_valid)\n",
    "    p_valid.append(yv)\n",
    "\n",
    "#Second run. Training on (X, y) and predicting on X_test.\n",
    "    clf.fit(X_tmp, y_tmp)\n",
    "    yt = clf.predict_proba(X_test)\n",
    "    yhat= clf.predict(X_test)\n",
    "    p_test.append(yt)\n",
    "\n",
    "#Printing out the performance of the classifier\n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'logloss  =>', log_loss(y_test, yt)))\n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'accuracy =>', accuracy_score(y_test, yhat)))\n",
    "    print('')\n",
    "# p_valid, p_test ****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hidden layer 1 to hidden layer 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "def objf_ens_optA(w, Xs, y, n_class=12):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensemble.\n",
    "    :param w: array-like, shape=(n_preds)\n",
    "        Candidate solution to the optimization problem (vector of weights)\n",
    "    :param Xs: list of predictions combine\n",
    "        Each prediction is the solution of an individual classifier and has a shape = (n_samples, n_classes)\n",
    "    :param y: array-like shape=(n_samples,)\n",
    "        Class labels\n",
    "    :param n_class: int\n",
    "        Number of classes in the problem (12 in Airbnb Data)\n",
    "\n",
    "    :return: score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    score = log_loss(y, sol)\n",
    "    return score\n",
    "\n",
    "class EN_optA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions X1,X2,...,Xn, computes the optimal set of weights w1,w2,...,wn\n",
    "    such that minimizes log_loss\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class=12):\n",
    "        super(EN_optA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving optimization problem\n",
    "        :param X: list of predictions to be ensemble\n",
    "            Each prediction is the solution of an individual classifiers\n",
    "            shape = (n_samples, n_classes).\n",
    "        :param y: array-like\n",
    "            Class labels\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        x0 = np.ones(len(Xs)) # initial equal weights for all predictions\n",
    "        bounds = [(0,1)] * len(x0)\n",
    "        cons = ({'type':'eq','fun': lambda w: 1-sum(w)})\n",
    "        res = minimize(objf_ens_optA, x0, args=(Xs, y, self.n_class),\n",
    "                       method='SLSQP',\n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "\n",
    "    def preidct_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        :param X: list of predictions to be blended.\n",
    "        :return: array-like, shape=(n_samples, n_class)\n",
    "            The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i]\n",
    "        return y_pred\n",
    "\n",
    "def objf_ens_optB(w, Xs, y, n_class=12):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensemble\n",
    "    :param w: array-like, shape=(n_preds)\n",
    "        Candidate solution to the optimization problem (vector of weights)\n",
    "    :param Xs: list of predictions to combine\n",
    "        Each prediction is the solution of an individual classifier and has\n",
    "        shape=(n_samples, n_classes).\n",
    "    :param y: array-like shape = (n_sample, )\n",
    "        Class labels\n",
    "    :param n_class: int\n",
    "        Number of classes in the problem, i.e. = 12\n",
    "    :return: score of the candidate solution\n",
    "    \"\"\"\n",
    "    # set constrains ==> sum of weights is 1, in another way.\n",
    "    w_range = np.arange(len(w)) % n_class\n",
    "    for i in range(n_class):\n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:,1 % n_class] += Xs[int(i / n_class)][: , i % n_class] * w[i]\n",
    "\n",
    "    score = log_loss(y, sol)\n",
    "    return score\n",
    "\n",
    "class EN_optB(BaseEstimator):\n",
    "    \"\"\"\n",
    "    The class find the optimal solution of weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_calss = 12):\n",
    "        super(EN_optB, self).__init__()\n",
    "        self.n_class = n_calss\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        x0 = np.ones(self.n_class * len(Xs) / float(len(Xs)))\n",
    "        bounds = [(0,1)] * len(x0)\n",
    "        res = minimize(objf_ens_optB, x0, args=(Xs, y, self.n_class),\n",
    "                       method='L-BFBS-B',\n",
    "                       bounds=bounds,\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "\n",
    "    def preidct_proba(self, X):\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Layer combine different classifier solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def objf_ens_optA(w, Xs, y, n_class=12):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensemble.\n",
    "    :param w: array-like, shape=(n_preds)\n",
    "        Candidate solution to the optimization problem (vector of weights)\n",
    "    :param Xs: list of predictions combine\n",
    "        Each prediction is the solution of an individual classifier and has a shape = (n_samples, n_classes)\n",
    "    :param y: array-like shape=(n_samples,)\n",
    "        Class labels\n",
    "    :param n_class: int\n",
    "        Number of classes in the problem (12 in Airbnb Data)\n",
    "\n",
    "    :return: score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    score = log_loss(y, sol)\n",
    "    return score\n",
    "\n",
    "class EN_optA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions X1,X2,...,Xn, computes the optimal set of weights w1,w2,...,wn\n",
    "    such that minimizes log_loss\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class=12):\n",
    "        super(EN_optA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving optimization problem\n",
    "        :param X: list of predictions to be ensemble\n",
    "            Each prediction is the solution of an individual classifiers\n",
    "            shape = (n_samples, n_classes).\n",
    "        :param y: array-like\n",
    "            Class labels\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        x0 = np.ones(len(Xs)) # initial equal weights for all predictions\n",
    "        bounds = [(0,1)] * len(x0)\n",
    "        cons = ({'type':'eq','fun': lambda w: 1-sum(w)})\n",
    "        res = minimize(objf_ens_optA, x0, args=(Xs, y, self.n_class),\n",
    "                       method='SLSQP',\n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "\n",
    "    def preidct_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        :param X: list of predictions to be blended.\n",
    "        :return: array-like, shape=(n_samples, n_class)\n",
    "            The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i]\n",
    "        return y_pred\n",
    "\n",
    "def objf_ens_optB(w, Xs, y, n_class=12):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensemble\n",
    "    :param w: array-like, shape=(n_preds)\n",
    "        Candidate solution to the optimization problem (vector of weights)\n",
    "    :param Xs: list of predictions to combine\n",
    "        Each prediction is the solution of an individual classifier and has\n",
    "        shape=(n_samples, n_classes).\n",
    "    :param y: array-like shape = (n_sample, )\n",
    "        Class labels\n",
    "    :param n_class: int\n",
    "        Number of classes in the problem, i.e. = 12\n",
    "    :return: score of the candidate solution\n",
    "    \"\"\"\n",
    "    # set constrains ==> sum of weights is 1, in another way.\n",
    "    w_range = np.arange(len(w)) % n_class\n",
    "    for i in range(n_class):\n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:,1 % n_class] += Xs[int(i / n_class)][: , i % n_class] * w[i]\n",
    "\n",
    "    score = log_loss(y, sol)\n",
    "    return score\n",
    "\n",
    "class EN_optB(BaseEstimator):\n",
    "    \"\"\"\n",
    "    The class find the optimal solution of weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_calss = 12):\n",
    "        super(EN_optB, self).__init__()\n",
    "        self.n_class = n_calss\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        x0 = np.ones(self.n_class * len(Xs) / float(len(Xs)))\n",
    "        bounds = [(0,1)] * len(x0)\n",
    "        res = minimize(objf_ens_optB, x0, args=(Xs, y, self.n_class),\n",
    "                       method='L-BFBS-B',\n",
    "                       bounds=bounds,\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "\n",
    "    def preidct_proba(self, X):\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training, validating and testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st layer - individual classifiers\n",
    "Training on (X_train, y_train) and predicting on (X_valid)\n",
    "\n",
    "Training on (X, y) and predicting on (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "\n",
    "# import feature_engineering as fe\n",
    "# X_train, X_valid, X_test, X_tmp = fe.X_train, fe.X_valid, fe.X_test, fe.X_tmp\n",
    "# y_train, y_valid, y_test, y_tmp = fe.y_train, fe.y_valid, fe.y_test, fe.y_tmp\n",
    "\n",
    "\n",
    "#fixing random state\n",
    "random_state=1\n",
    "\n",
    "#Defining the classifiers\n",
    "clfs = {'LR'  : LogisticRegression(random_state=random_state),\n",
    "        'SVM' : SVC(probability=True, random_state=random_state),\n",
    "        'RF'  : RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                                       random_state=random_state),\n",
    "        'GBM' : GradientBoostingClassifier(n_estimators=50,\n",
    "                                           random_state=random_state),\n",
    "        'ETC' : ExtraTreesClassifier(n_estimators=100, n_jobs=-1,\n",
    "                                     random_state=random_state),\n",
    "        'KNN' : KNeighborsClassifier(n_neighbors=30)}\n",
    "\n",
    "#predictions on the validation and test sets\n",
    "p_valid = []\n",
    "p_test = []\n",
    "\n",
    "print('Performance of individual classifiers (1st layer) on X_test')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "for nm, clf in clfs.items():\n",
    "    #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "    clf.fit(X_train, y_train)\n",
    "    yv = clf.predict_proba(X_valid)\n",
    "    p_valid.append(yv)\n",
    "\n",
    "    #Second run. Training on (X, y) and predicting on X_test.\n",
    "    clf.fit(X_tmp, y_tmp)\n",
    "    yt = clf.predict_proba(X_test)\n",
    "    p_test.append(yt)\n",
    "\n",
    "    #Printing out the performance of the classifier\n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'logloss  =>', log_loss(y_test, yt)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd layers (XV, y_valid), (XT, y_test):\n",
    "Predictions on X_valid are used as training set (XV).\n",
    "\n",
    "Predictions on X_test are used as test set (XT). \n",
    "\n",
    "EN_optA, EN_optB and their calibrated versions are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 12\n",
    "\n",
    "print('Performance of optimization based ensemblers (2nd layer) on X_test')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "#Creating the data for the 2nd layer.\n",
    "XV = np.hstack(p_valid)\n",
    "XT = np.hstack(p_test)\n",
    "\n",
    "#EN_optA\n",
    "enA = EN_optA(n_classes)\n",
    "enA.fit(XV, y_valid)\n",
    "w_enA = enA.w\n",
    "y_enA = enA.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "\n",
    "#Calibrated version of EN_optA\n",
    "cc_optA = CalibratedClassifierCV(enA, method='isotonic')\n",
    "cc_optA.fit(XV, y_valid)\n",
    "y_ccA = cc_optA.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "\n",
    "#EN_optB\n",
    "enB = EN_optB(n_classes)\n",
    "enB.fit(XV, y_valid)\n",
    "w_enB = enB.w\n",
    "y_enB = enB.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "#Calibrated version of EN_optB\n",
    "cc_optB = CalibratedClassifierCV(enB, method='isotonic')\n",
    "cc_optB.fit(XV, y_valid)\n",
    "y_ccB = cc_optB.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd layer average of individual classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
